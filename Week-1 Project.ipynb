{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cmye8o3Zn29d"
   },
   "source": [
    "# Project Introduction\n",
    "\n",
    "In this project, we create a scraper program that visits a Website and reads their page contents to collects certain information.\n",
    "\n",
    "The main function of our scapper is to collect the product information of products being sold on Tiki, including products on every pages of all categories. The collected information will be saved as arrays, which will in turn be added into a Pandas dataframe.\n",
    "\n",
    "The product information includes:\n",
    "\n",
    "* Product ID\n",
    "* Seller ID\n",
    "* Product title\n",
    "* Price\n",
    "* URL of the product image\n",
    "* Product Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m-QU3Z7cpCg1"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ztDwuVy9g21i"
   },
   "source": [
    "### The HTML Parser\n",
    "This function will return the parsed HTML content from the given URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jGFSV2qfpHcJ"
   },
   "outputs": [],
   "source": [
    "# Parser function to retrieve and parse the HTML code of a website \n",
    "def parser(url):\n",
    "    \"\"\"Get a parsed version of an URL\"\"\"\n",
    "    \n",
    "    try:\n",
    "      # Retrieve plain HTML code.\n",
    "      plain = requests.get(url).text\n",
    "\n",
    "      # Parse the plain content into structured one\n",
    "      soup = BeautifulSoup(plain)\n",
    "\n",
    "      return soup\n",
    "    \n",
    "    except Exception as err:\n",
    "      print('There was a problem: {}'.format(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PsR5WsW3pb6T"
   },
   "source": [
    "### _Test the function_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EE64bE3IpSmu"
   },
   "outputs": [],
   "source": [
    "url = 'https://tiki.vn/'\n",
    "s = parser(url)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0KyouD51pkAq"
   },
   "source": [
    "### Collect Main Category links\n",
    "This function will collect all category URLs from tiki.vn website based on its main category navigation bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pCmgG2a1pp32"
   },
   "outputs": [],
   "source": [
    "def get_category_urls():\n",
    "    \"\"\"Get the URLs of all categories on Tiki.vn\"\"\"\n",
    "    \n",
    "    url = \"https://tiki.vn\"\n",
    "    \n",
    "    # Get the homepage's html in BeautifulSoup format\n",
    "    soup = parser(url)\n",
    "    \n",
    "    # Initialize an empty list of category \n",
    "    category_list = []\n",
    "\n",
    "    # Scrape through the main category navigation bar\n",
    "    for i in soup.find_all('li', class_=\"MenuItem-tii3xq-0\"):\n",
    "      \n",
    "        # Get the category value\n",
    "        category = i.a.find('span', class_='text').text\n",
    "        \n",
    "        # Get the url value\n",
    "        url = i.a[\"href\"]\n",
    "        \n",
    "        # Add category and url values to list\n",
    "        category_list.append((category, url))\n",
    "        \n",
    "    return category_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KrSy2xE6qpgD"
   },
   "source": [
    "### _Test the function_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nkq-JOI2qrQL"
   },
   "outputs": [],
   "source": [
    "url_list = get_category_urls()\n",
    "url_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cpGAmuUHqtSF"
   },
   "source": [
    "### The Product Scraper\n",
    "\n",
    "The scraper will parse the product page from the given URL and return a list of products. If there is no products, it will return an empty list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SymHKkh_q05l"
   },
   "outputs": [],
   "source": [
    "def scrape_products(cat, url):\n",
    "    \"\"\"Scrape product information of all products on one page\"\"\"\n",
    "    \n",
    "    # Initialize empty 'results' list\n",
    "    results = []\n",
    "   \n",
    "    # Get the parsed html code\n",
    "    soup = parser(url)\n",
    "    \n",
    "    # Find all products on this page\n",
    "    product_items = soup.find_all('div', class_='product-item')\n",
    "  \n",
    "    # If there is no products, return an empty list.\n",
    "    if len(product_items) == 0:\n",
    "      return []\n",
    "    \n",
    "    # If the page has products\n",
    "    else: \n",
    "        \n",
    "        # Iterate through all product_items and store the product information in the 'row' list\n",
    "        for product in product_items:\n",
    "          \n",
    "            row = [product.get('data-id'), \n",
    "                   product.get('data-seller-product-id'), \n",
    "                   product.get('data-title'),\n",
    "                   product.get('data-price'),\n",
    "                   product.img['src'], \n",
    "                   cat]   \n",
    "\n",
    "            # Add the product information of each product into the 'results' list\n",
    "            results.append(row)\n",
    "              \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E8M_cZeLr_lZ"
   },
   "source": [
    "### _Test the function_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iEvVX2DNsCQx"
   },
   "outputs": [],
   "source": [
    "test_scraper = scrape_products(url_list[0][0], url_list[0][1])\n",
    "test_scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "piuRh2ZJsE_R"
   },
   "source": [
    "### The Main Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p9YBoa--sWJg"
   },
   "outputs": [],
   "source": [
    "def scrape_all():\n",
    "    \"\"\"Scrape all products on Tiki!\n",
    "    \"\"\"\n",
    "    print('INFO scrape_all(): Start craping')\n",
    "    \n",
    "    # Get all category links\n",
    "    queue = get_category_urls()\n",
    "  \n",
    "    # Initialize the results, which will store all products information from the scraper\n",
    "    list_all_products = []\n",
    "    \n",
    "    # Initialize the 'page' variable, which indicates the current product page of the current category\n",
    "    page = 1\n",
    "    \n",
    "    # While there are links in the queue, we will run through each link and get the products\n",
    "    while len(queue) > 0:\n",
    "      \n",
    "      # We will proceed from the last link in the queue        \n",
    "      url = queue[-1][1]\n",
    "      cat = queue[-1][0]\n",
    "      \n",
    "      # Check to keep the original category's url and its category name\n",
    "      if \"page\" not in url:\n",
    "        url_orig = url\n",
    "        cat_orig = cat\n",
    "        \n",
    "      # Remove the last link in queue so that new product url from page 2 will be added at the end of the queue\n",
    "      queue.pop() \n",
    "      \n",
    "      print('Scraping', cat_orig + \" page \" + str(page))\n",
    "      print(url)\n",
    "      \n",
    "      # Get the list of products of the current page and store it in a temporary variable\n",
    "      list_current_products = scrape_products(cat, url)\n",
    "      \n",
    "      # If the page has products, we will create the next product page link and add it to the queue\n",
    "      if len(list_current_products) > 0:\n",
    "        \n",
    "        # Add the products from new_rows to the results list\n",
    "        list_all_products += list_current_products\n",
    "        \n",
    "        # Generate next page url and add it to the end of list `queue` so that it will be the next link to be scraped\n",
    "        page += 1\n",
    "        url = url_orig + \"&page=\" + str(page)\n",
    "\n",
    "        # Add the new page url to the end of list `queue`\n",
    "        queue.append([cat_orig, url]) \n",
    "            \n",
    "        print('Add next page', page)\n",
    "      else: \n",
    "        # Now the product page link doesn't return any product, which indicates that we have done getting all products...\n",
    "        # ...of the current category. We will reset the page number to 1 in order to scrape the next category\n",
    "        page = 1\n",
    "        \n",
    "    # Return the final list of all products\n",
    "    return list_all_products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hLPbcWDrzGda"
   },
   "source": [
    "### _Test the function_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZoLJpjQezHwE"
   },
   "outputs": [],
   "source": [
    "tiki_products = scrape_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6GSv7laEz5Yk"
   },
   "outputs": [],
   "source": [
    "print(len(tiki_products))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WYjXO7AB0eB2"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tiki_products, columns = ['product_id', 'seller_id', 'title', 'price', 'image_url', 'category'])\n",
    "df.sample(10)\n",
    "#df.to_csv(\"tiki_all_products.csv\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zYf3s98CzTUE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4fnMfRYdh8q_"
   },
   "source": [
    "# BONUS: Our 2nd Solution\n",
    "We came up with another solution, which is to collects all URLs of categories and then will loop through each category URL one by one to get the products.\n",
    "\n",
    "For the convenience, we put all code within one single cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D20BrOqOiSGu"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "'''*****************************************************************************\n",
    "'''\n",
    "def parser(url):\n",
    "    \"\"\"Parser function to retrieve and parse the HTML code of a website \n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "      # Retrieve plain HTML code.\n",
    "      plain = requests.get(url).text\n",
    "\n",
    "      # Parse the plain content into structured one\n",
    "      soup = BeautifulSoup(plain)\n",
    "\n",
    "      return soup\n",
    "    \n",
    "    except Exception as err:\n",
    "      print('There was a problem: {}'.format(err))\n",
    "\n",
    "      \n",
    "'''******************************************************************************\n",
    "'''\n",
    "def get_category_urls():\n",
    "    \"\"\"Get the URLs of all categories on Tiki.vn\"\"\"\n",
    "    \n",
    "    url = \"https://tiki.vn\"\n",
    "    list_category_urls = []\n",
    "    \n",
    "    # Get the homepage's html in BeautifulSoup format\n",
    "    soup = parser(url)\n",
    "\n",
    "    # Scrape through the main category navigation bar\n",
    "    for i in soup.find_all('li', class_=\"MenuItem-tii3xq-0\"):\n",
    "      \n",
    "        # Get the category value\n",
    "        category = i.a.find('span', class_='text').text\n",
    "        \n",
    "        # Get the url value\n",
    "        url = i.a[\"href\"]\n",
    "        \n",
    "        # Add category and url values to list\n",
    "        list_category_urls.append((category, url))\n",
    "        \n",
    "    return list_category_urls\n",
    "\n",
    "  \n",
    "\n",
    "'''******************************************************************************\n",
    "'''\n",
    "def scrape_all_products(cat_name, cat_url):\n",
    "  '''This function will scrape through all product pages in given category URL\n",
    "     and return the list of products\n",
    "  '''\n",
    "  \n",
    "  all_products = []\n",
    "  page = 1\n",
    "  \n",
    "  # Make the product page url template\n",
    "  product_page_url = cat_url + '&page='\n",
    "  \n",
    "  # Now we will go through each product page, if it exists, then we will scrape the products\n",
    "  while test_prod_page_url(product_page_url + str(page)):\n",
    "    print('Scraping {} page {}'.format(cat_name, page))\n",
    "      \n",
    "    all_products += scrape_page(product_page_url + str(page))\n",
    "    page += 1\n",
    "    \n",
    "  # Return the products\n",
    "  return all_products\n",
    "\n",
    "\n",
    "\n",
    "'''******************************************************************************\n",
    "'''\n",
    "def test_prod_page_url(url_page):\n",
    "  \"\"\"This function will check if a product page url exists on Tiki.vn or not\n",
    "  \"\"\"\n",
    "  \n",
    "  # Get the HTML document\n",
    "  try:\n",
    "    res = requests.get(url_page)\n",
    "    \n",
    "    #Detect if this is a real product url by searching for the result html tag\n",
    "    return \"<h4 name=\\\"results-count\\\">\" in res.text \n",
    "  \n",
    "  except Exception as err:\n",
    "    print('There was a problem: {}'.format(err))\n",
    "\n",
    "    \n",
    "    \n",
    "'''******************************************************************************\n",
    "'''\n",
    "def scrape_page(url):\n",
    "  \"\"\"This function will scrape the products of one given product page\n",
    "  \"\"\"\n",
    "  \n",
    "  # Get the HTML document  \n",
    "  soup = parser(url)\n",
    "  \n",
    "  products = []\n",
    "  \n",
    "  # Get the list of articles   \n",
    "  soup_products = soup.find_all(\"div\", class_='product-item')\n",
    "  \n",
    "  for item in soup_products:\n",
    "    product = [ item.get('data-id'), \n",
    "                item.get('data-seller-product-id'), \n",
    "                item.get('data-title'),\n",
    "                item.get('data-price'),\n",
    "                item.img['src'], \n",
    "                item.get('data-category') ]\n",
    "    \n",
    "    products.append(product)\n",
    "  return products\n",
    "\n",
    "\n",
    "\n",
    "'''******************************************************************************\n",
    "'''\n",
    "def get_tiki_products():\n",
    "  \"\"\"The main function to get all products on Tiki.vn\n",
    "  \"\"\"\n",
    "  tiki_products = []\n",
    "\n",
    "  # First, get all category URLs\n",
    "  list_category_urls = get_category_urls()\n",
    "\n",
    "  # Now loop through each category and collect the products\n",
    "  for url in list_category_urls:\n",
    "      tiki_products += scrape_all_products(url[0], url[1])\n",
    "\n",
    "  return tiki_products\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l5SQU6FYxGWL"
   },
   "source": [
    "### _Test the function_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X0oK5OwcvcmT"
   },
   "outputs": [],
   "source": [
    "tiki_products = get_tiki_products()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hBJoozlxwSHV"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tiki_products, columns = ['product_id', 'seller_id', 'title', 'price', 'image_url', 'category' ])\n",
    "df.sample(10)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Week-1 Project.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
